# SubSec Environment Configuration
# Copy this file to .env and customize as needed

# ============================================================
# Model Selection
# ============================================================
# Specify which Granite-4 model to use
# Options:
#   - ibm-granite/granite-4.0-micro-base  (dense attention, base/pretrained)
#   - ibm-granite/granite-4.0-micro       (dense attention, instruction-tuned)
#   - ibm-granite/granite-4.0-h-micro-base (hybrid Mamba+Attention, base)
#   - ibm-granite/granite-4.0-h-micro      (hybrid Mamba+Attention, instruction-tuned)
#   - ibm-granite/granite-4.0-h-tiny-base
#   - ibm-granite/granite-4.0-h-tiny
#   - ibm-granite/granite-4.0-h-small-base
#   - ibm-granite/granite-4.0-h-small
MODEL_NAME_OR_PATH=ibm-granite/granite-4.0-micro

# ============================================================
# Context Management (SubSec Optimization Knobs)
# ============================================================

# Maximum prompt tokens to send to model (bounds TTFT)
# Lower values = faster TTFT, but may truncate very long contexts
# Recommended: 1024-2048 for balanced latency/quality
# Default: 1024
SUBSEC_MAX_PROMPT_TOKENS=1024

# Number of recent conversation turns to keep in full detail
# Each turn = 1 user message + 1 assistant response
# Older turns are compressed into a summary
# Recommended: 2-3 for optimal TTFT, 4-5 for better context retention
# Default: 2
SUBSEC_RECENT_TURNS_FULL=2

# Maximum characters per message in the compressed summary
# Controls summary verbosity vs compactness
# Recommended: 120-200 for good balance
# Default: 160
SUBSEC_SUMMARY_CHARS_PER_MSG=160

# Maximum lines in the compressed summary
# Bounds total summary length
# Recommended: 10-15
# Default: 12
SUBSEC_SUMMARY_MAX_LINES=12

# ============================================================
# Generation Parameters
# ============================================================

# Maximum new tokens to generate per turn
# Lower values = faster response time, higher values = more detailed answers
# Recommended: 512 for balanced responses, 1024-2048 for long-form
# Default: 512
SUBSEC_MAX_NEW_TOKENS=512

# ============================================================
# Model Loading & Architecture
# ============================================================

# Whether to trust remote model code (required for some hybrid models)
# Options: 0, 1, true, false, yes, no
# Default: auto-detected (enabled for hybrid models)
# SUBSEC_TRUST_REMOTE_CODE=1

# ============================================================
# Performance Tuning (Advanced)
# ============================================================

# Enable expanded CUDA memory segments to reduce fragmentation
# Helpful for hybrid models with complex memory patterns
# Options: true, false
# Default: handled automatically
# PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
